{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oyjuffer/DL-Final/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "HbPKwGsmbFE0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "random_seed = 40"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# File loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir= \"emotion-detection-fer/\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data loading\n",
        "Load train and validation sets.Images are resized to 96x96 pixels, as this is the smallest size available for the MobileNetV2 model. Even though the images are grayscale, they are loaded as RGB as mobilenet requires 3 channels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJYEs9WuZxdH",
        "outputId": "9c66c615-1af2-4ea6-e90e-616c08412fb9"
      },
      "outputs": [],
      "source": [
        "image_size = 96 # Images are 48, but smallest model is 96\n",
        "batch_size = 512 # default batch size\n",
        "\n",
        "ext_flag=0\n",
        "\n",
        "if ext_flag:\n",
        "\n",
        "  train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir + \"train_ext/\",\n",
        "    image_size=(image_size, image_size),\n",
        "    batch_size=batch_size,\n",
        "    color_mode='rgb')\n",
        "  \n",
        "  val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir + \"val_ext/\",\n",
        "    image_size=(image_size, image_size),\n",
        "    batch_size=batch_size,\n",
        "    color_mode='rgb')\n",
        "  \n",
        "else:\n",
        "  \n",
        "  train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir + \"train/\",\n",
        "    image_size=(image_size, image_size),\n",
        "    batch_size=batch_size,\n",
        "    color_mode='rgb')\n",
        "\n",
        "  train_ds, val_ds = tf.keras.utils.split_dataset(train_ds, left_size=0.8, seed=random_seed)\n",
        "\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir + \"test/\",\n",
        "  image_size=(image_size, image_size),\n",
        "  batch_size=batch_size,\n",
        "  color_mode='rgb')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### List the different classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmRy53_wY3C8",
        "outputId": "392bf2fe-9004-4414-f7c2-0be6df16fe8f"
      },
      "outputs": [],
      "source": [
        "class_names = test_ds.class_names\n",
        "print(class_names)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emnBoxJEgle3"
      },
      "outputs": [],
      "source": [
        "# Optional, just to get an understanding of whats happening\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in val_ds.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transfer Learning\n",
        "First, the input is preprocessed to convert the values in the images, which are between [0, 255], to values between [-1, 1]. This is done to match the expected input of the MobileNetV2 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the MobileNetV2 model, with the weights pre-trained on ImageNet. The model is set to not trainable, as we only want to use the convolutional layers for feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsJzzfBuha8v"
      },
      "outputs": [],
      "source": [
        "IMG_SHAPE = (image_size, image_size, 3)\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model.trainable = False"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Show model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add classification layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "feature_batch_average = global_average_layer(feature_batch)\n",
        "print(feature_batch_average.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_classes = len(class_names)\n",
        "prediction_layer = tf.keras.layers.Dense(num_classes)\n",
        "prediction_batch = prediction_layer(feature_batch_average)\n",
        "print(prediction_batch.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputs = tf.keras.Input(shape=(image_size, image_size, 3))\n",
        "x = inputs\n",
        "x = preprocess_input(x)\n",
        "x = base_model(x, training=False)\n",
        "x = global_average_layer(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = prediction_layer(x)\n",
        "model = tf.keras.Model(inputs, outputs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compile model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0ik4qsMhwT7"
      },
      "outputs": [],
      "source": [
        "base_learning_rate = 0.0001\n",
        "\n",
        "model.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "  metrics=['accuracy'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Show model architecture again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional, just to get an understanding of whats happening\n",
        "model.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, train=train_ds, validation=val_ds, epochs=10, patience=4, workers=4):\n",
        "  early_stop = EarlyStopping(monitor=\"val_loss\", patience=patience)\n",
        "  \n",
        "  callbacks = [early_stop]\n",
        "\n",
        "  history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=callbacks,\n",
        "    epochs=epochs,\n",
        "    workers=workers\n",
        "  )\n",
        "  \n",
        "  return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fB_MSIedhxBK"
      },
      "outputs": [],
      "source": [
        "history = train_model(model, epochs=200, patience=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15/15 [==============================] - 19s 1s/step - loss: 3.6122 - accuracy: 0.6017\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(\"cnn_trained_transfer\", overwrite=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_ds)\n",
        "\n",
        "labels = []; predictions = []\n",
        "for element in test_ds.as_numpy_iterator():\n",
        "    preds = model.predict(element[0], batch_size=batch_size, verbose=0)\n",
        "    \n",
        "    for ii in range(len(preds)):\n",
        "        labels.append(class_names[element[1][ii]])\n",
        "        predictions.append(class_names[preds[ii].argmax()])\n",
        "        \n",
        "cm = confusion_matrix(labels, predictions, labels=class_names, normalize='pred')\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "disp.plot()\n",
        "\n",
        "plt.savefig('cnn_confussion_matrix.png')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "# plt.ylim([min(plt.ylim()),0.6])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
